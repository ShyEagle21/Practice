{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "\n",
    "# Note: If you want to run this code to test your solution, you will need to install SciKit Learn and Numpy\n",
    "# This can be done with: pip install scikit-learn numpy\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(X, y, phi, lmbd):\n",
    "    '''\n",
    "    Fits a logistic regression model on the data (X, y), using the parameters\n",
    "    phi and lmbd. Returns the learned regression coefficients theta\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X: np.array\n",
    "        2-D feature matrix\n",
    "    y: np.array\n",
    "        1-D array of targets\n",
    "    phi: np.array\n",
    "        Feature mask used to select subset of features \n",
    "        e.g., if d=3, phi= [True, False, True] would select the first and last features\n",
    "    lmbd: float\n",
    "        Regularization penalty weight\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    theta: sklearn.LogisticRegression model\n",
    "        Trained logistic regression model, with properties .coef_ and .intercept_\n",
    "    '''\n",
    "    if lmbd == 0: # no regularization\n",
    "        theta = LogisticRegression(penalty='none')\n",
    "    else:\n",
    "        # C is inverse of lambda\n",
    "        theta = LogisticRegression(penalty='l2', C=1/lmbd)\n",
    "    \n",
    "    # Assuming phi is feature mask\n",
    "    theta.fit(X[:, phi], y)\n",
    "    # cofficients of model can be access by theta.coef_ and theta.intercept_\n",
    "    return theta\n",
    "\n",
    "def predict(X, phi, theta):\n",
    "    '''\n",
    "    Returns predictions for given model theta on phi(X).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X: np.array\n",
    "        2-D feature matrix\n",
    "    phi: np.array\n",
    "        Feature mask used to select subset of features \n",
    "        e.g., if d=3, phi= [True, False, True] would select the first and last features\n",
    "    theta: sklearn.LogisticRegression\n",
    "        Trained logistic regression model, with properties .coef_ and .intercept_\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    np.array\n",
    "        1-D array of predictions of model theta on X for subset of features \n",
    "        indicated by feature mask phi\n",
    "    '''\n",
    "    return theta.predict(X[:, phi])\n",
    "\n",
    "\n",
    "def logloss(y, y_hat):\n",
    "    '''\n",
    "    Returns logistic loss between targets y and predictions y_hat.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y: np.array\n",
    "        1-D array of ground truth targets\n",
    "    y_hat: np.array\n",
    "        1-D array of predictions\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    float\n",
    "        Logistic loss between targets y and predictions y_hat\n",
    "    '''\n",
    "    return log_loss(y, y_hat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100\n",
    "d = 5\n",
    "X = np.random.normal(size=(N, d))\n",
    "y = np.random.choice([0, 1], size=N)\n",
    "lmbds = [0, 0.1, 1, 10]\n",
    "phis = [np.random.choice(a=[True, False], size=d) for i in range(3)]\n",
    "# make sure at least one feature from each feature mask is selected\n",
    "for phi in phis:\n",
    "    phi[0] = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=4)\n",
    "print ('Train set:', X_train.shape,  y_train.shape)\n",
    "print ('Test set:', X_test.shape,  y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sweep_hyperparameters(X_train, y_train, X_val, y_val, lmbds, phis):\n",
    "    '''\n",
    "    Finds the best settings of lambda and phi, and trains a logistic regression model with these parameters.\n",
    "\n",
    "    You need to decide what other variables to pass as input arguments. (i.e., what split(s) of data to use)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train: np.array\n",
    "        2-D matrix of training data features\n",
    "    y_train: np.array\n",
    "        1-D array of train targets\n",
    "    X_val: np.array\n",
    "        2-D matrix of validation data features\n",
    "    y_val: np.array\n",
    "        1-D array of validation targets\n",
    "    lmbds: list\n",
    "        List of possible settings of regularization term lambda to consider.\n",
    "    phis: list\n",
    "        List of feature masks to consider. \n",
    "        Note: a feature mask is a boolean array (e.g., [True, False, True]) that\n",
    "        specifies what features to use vs ignore.\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    best_lmbd: float\n",
    "        chosen regularization parameter lambda\n",
    "    best_phi: np.array\n",
    "        chosen feature mask (i.e., feature subset) phi\n",
    "    best_theta: sklearn.LogisticRegression model\n",
    "        Logistic regresion model trained using best_lmbd and best_phi\n",
    "    '''\n",
    "    # TODO: Your Code Goes Here\n",
    "    loss_tracker = 999999999999\n",
    "    best_phi = 0\n",
    "    best_lmbd = 0\n",
    "    for lamb in lmbds:\n",
    "        for phi in phis:\n",
    "            reg = fit(X_train, y_train, phi, lamb)\n",
    "            pred = predict(X_test, phi, reg)\n",
    "            loss = logloss(y_test,pred)\n",
    "            print(f\"lambda is {lamb}\")\n",
    "            print(f\"phi is {phi}\")\n",
    "            print(f\"loss is {loss}\")\n",
    "            if loss < loss_tracker:\n",
    "                    loss_tracker = loss\n",
    "                    best_phi = phi\n",
    "                    best_lmbd = lamb\n",
    "                    best_theta = reg\n",
    "\n",
    "    print(f\"the best phi is {best_phi}\")\n",
    "    print(f\"the best lambda is {best_lmbd}\")\n",
    "    print(f\"the best loss is {loss}\")\n",
    "    print(f\"the best theta is {best_theta}\")   \n",
    "    return best_lmbd, best_phi, best_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "practice_vals = sweep_hyperparameters(X_train, y_train, X_test, y_test, lmbds, phis)\n",
    "print(practice_vals)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
